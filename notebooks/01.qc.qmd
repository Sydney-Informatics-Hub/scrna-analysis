---
title: "scRNA Analysis - 01 - QC"
author: "Sydney Informatics Hub"
format: html
---

The goal of this notebook is for you to be able to plot and inspect some basic quality control metrics for your single cell sequencing data. We start by loading your samples into R with the Seurat package and inspecting the general distribution of counts. We then identify read and gene count thresholds to use for filtering your data and use a clustering approach to identify groups of cells of potentially poor quality that we want to remove before proceeding to downstream analyses.

## Instructions

Quality control is inherently a manual process. This notebook is desiged to be worked through interactively. Run each block of code and inspect the output. In a few places, you will need to create a CSV or TSV file for importing your data and metadata about your samples, including filtering thresholds. In other places, we have developed some interactive R Shiny apps that will help you quickly determine parameters to set for further analysis.

Once you have worked through the entire notebook, we recommend rendering it into an HTML file that will serve as a record of how you processed your samples.

## Imports

While working through this notebook manually, you will need to run the following block of code to import all the necessary R libraries and helper functions:

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(Seurat)
library(hdf5r)
library(glmGamPoi)
library(clustree)
library(celldex)
library(SingleR)
source("R/helpers.R")
source("R/shinyapps.R")
```

## Define inputs {#define_inputs}

This notebook has been designed to be run after initial alignment and counting with the `nf-core/scrnaseq` Nextflow pipeline. This pipeline outputs one `.Rds` file containing a Seurat data object per sample.

In the following block, we load a CSV sample sheet `inputs/samplesheet.csv` which defines your samples and any metadata you would like to annotate them with. A template file already exists at this location; you can modify it to point to your single cell data, add the appropriate sample IDs and metadata columns. Then run the following block of code to read in your data and annotate it. The CSV file should have at least two columns, `sample` and `rds_path`. Additional columns will be treated as metadata variables that will be added to the Seurat objects. The value under the `sample` column will be used as the sample's name in the Seurat object. The value under the `rds_path` column should be the path to the RDS file generated by `nf-core/scrnaseq`. For example:

```
sample,rds_path,tumour
normal_sample,/path/to/normal_sample.rds,neg
tumour_sample,/path/to/tumour_sample.rds,pos
```

```{r inputs}
samples <- read_csv("inputs/samplesheet.csv")
samples
```

### Generate input files for downstream steps

The following code chunk will generate all the input files for the downstream processing steps in this notebook and the following notebooks.

Most of these files will be templates filled with placeholder values that you will need to update based on the QC results below.

```{r generate_input_files}
cluster_filter_samplesheet <- "inputs/cluster_filter.tsv"
cluster_res_samplesheet <- "inputs/cluster_resolutions.tsv"
doublet_samplesheet <- "inputs/doublet_samples.tsv"

tmp_df <- samples
tmp_df <- tmp_df %>%
  mutate(
    res = 1.0,
    clusters_to_remove = "0,1,2",
    multiplet_rate = ""
  )

tmp_df %>%
  select(sample, res, clusters_to_remove) %>%
  write_tsv(cluster_filter_samplesheet)

tmp_df %>%
  select(sample, res) %>%
  write_tsv(cluster_res_samplesheet)

tmp_df %>%
  select(sample, multiplet_rate) %>%
  write_tsv(doublet_samplesheet)

rm(tmp_df)
```

### Read in RDS data

We now read in all of your samples and store them as a list of Seurat objects.

```{r read_inputs}
all_seurat_objects <- lapply(transpose(samples), function(x) {
  md <- x[! names(x) %in% c("sample", "rds_path")]
  readRDS_update_metadata(
    path_to_rds = x$rds_path,
    sample_name = x$sample,
    metadata = md
  )
})
names(all_seurat_objects) <- sapply(all_seurat_objects, function(s) { as.character(s@meta.data$orig.ident[[1]]) })

all_metadata <- get_metadata_df(all_seurat_objects)
```

## Initial QC

First, we can check the number of cells each sample has:

```{r total_cells}
total_cells <- lapply(all_seurat_objects, function(x) table(x$orig.ident)) %>% unlist
tibble(sample = names(total_cells), n_cells = total_cells)
```

Next, plot the distribution of nCount_RNA, nFeature_RNA, and (if available), percent.mt for each sample:

```{r metadata_distributions}
log_y <- FALSE  # Set to `TRUE` if you want log-scaled plots
jitter <- FALSE  # Set to `TRUE` if you want to plot the individual data points

md_cols <- c("nCount_RNA", "nFeature_RNA")
if ("percent.mt" %in% colnames(all_metadata)) {
  md_cols <- c(md_cols, "percent.mt")
}

p_md <- all_metadata %>%
  pivot_longer(cols = md_cols) %>%
  ggplot(aes(x = orig.ident, y = value, fill = orig.ident, colour = orig.ident)) +
  facet_wrap(~ name, scales = "free_y") +
  theme_light() +
  theme(legend.position = "none")

if (log_y) {
  p_md <- p_md + scale_y_log10()
}
if (jitter) {
  p_md <- p_md + geom_jitter(size = 0.1, alpha = 0.7, colour = "lightgrey")
}

p_md + geom_violin(alpha = 0.1) 
```

### Identify filtering thresholds

The following Shiny app will help you determine thresholds to use for filtering your data. Play with the thresholds until you are happy with the data quality, then note the final thresholds for use in the following code block.

```{r app_qc_thresholds, eval = FALSE}
app_qc_thresholds(all_metadata)
```

In the following chunk, set your desired thresholds for each of the values. This will also generate a plot of your filtered data for you to inspect prior to committing.

```{r set_thresholds}
# These defaults will result in no filtering being applied
# TODO: Set your desired thresholds
qc_thresholds <- list()
qc_thresholds$ncount_lower <- 0
qc_thresholds$nfeature_lower <- 0
qc_thresholds$mtpercent_upper <- 100

# You generally won't want to change the upper count and lower MT percentage thresholds
# unless you identify outlying cells that should be removed
qc_thresholds$ncount_upper <- Inf
qc_thresholds$nfeature_upper <- Inf
qc_thresholds$mtpercent_lower <- 0

plot_filtered(all_metadata, qc_thresholds)
```

Apply the filtering thresholds:

```{r add_thresholds}
all_seurat_objects <- lapply(all_seurat_objects, function(s) {
  s2 <- s
  s2@meta.data <- s2@meta.data %>%
  mutate(
    qc_threshold = case_when(
      percent.mt >= qc_thresholds$mtpercent_lower &
        percent.mt <= qc_thresholds$mtpercent_upper &
        nFeature_RNA >= qc_thresholds$nfeature_lower &
        nFeature_RNA <= qc_thresholds$nfeature_upper &
        nCount_RNA >= qc_thresholds$ncount_lower &
        nCount_RNA <= qc_thresholds$ncount_upper ~ "keep",
      .default = "remove"
    )
  )
  s2
})

all_filtered_seurat_objects <- lapply(all_seurat_objects, function(s) {
  subset(s, subset = (qc_threshold == "keep"))
})

n_cells_pre_filtered <- sapply(all_seurat_objects, function(s) { nrow(s@meta.data) })
n_cells_post_filtered <- sapply(all_filtered_seurat_objects, function(s) { nrow(s@meta.data) })

n_cells_pre_filtered <- tibble(sample = names(n_cells_pre_filtered), n_cells_pre_filtered = n_cells_pre_filtered)
n_cells_post_filtered <- tibble(sample = names(n_cells_post_filtered), n_cells_post_filtered = n_cells_post_filtered)

filter_summary <- n_cells_pre_filtered %>% left_join(n_cells_post_filtered)
filter_summary
```

## SCTransform and initial clustering for further filtering

Now that we have applied some basic quality filters, we will now run the `SCTransform` method in Seurat, which is a critical pre-processing step that normalises and corrects the single cell count data.

We also want to perform some initial clustering of our single cell data to look for potentially poor-quality groups of cells that we should remove before running any analyses. To do this, we will use Seurat's `FindClusters` method. This method first constructs a nearest-neighbours graph of the data, identifying which cells are closely related to each other in terms of gene expression. Then this graph is used to detect "communities" of cells, using one of two main algorithms.

The original and default clustering algorithm that Seurat uses is the Louvain method. This has some pitfalls which has led to it being superceded by the Leiden algorithm. We recommend using this updated method, however it requires first ensuring that you have the `leidenalg` Python package installed.

In the following code chunk, set the `cluster_method` variable to either `"LOUVAIN"` or `"LEIDEN"`. We will also check if you are able to use the Leiden method; if you don't have the appropriate packages installed, the following code will error when trying `cluster_method <- "LEIDEN"`.

```{r set_cluster_method}
cluster_method <- "LOUVAIN"  # Set this to "LEIDEN" to use the more up-to-date method

stopifnot(cluster_method %in% c("LOUVAIN", "LEIDEN"))

can_use_leiden <- system("python3 -c 'import leidenalg'", ignore.stderr = TRUE) == 0

if (cluster_method == "LOUVAIN" && can_use_leiden) {
  warning("You have chosen to use the older Louvain clustering algorithm, but you can use the more up-to-date Leiden method, which we recommend. Please consider updating the `cluster_method` variable to 'LEIDEN'.")
} else if (cluster_method == "LEIDEN" && !can_use_leiden) {
  stop("You have chosen to use the up-to-date Leiden clustering algorithm, but you do not have the required `leidenalg` Python package installed. If you wish to use the Leiden method, please ensure you have Python installed as well as the `leidenalg` package. You can install this package with: `pip install leidenalg`. Otherwise, please change the `cluster_method` variable to 'LOUVAIN'.")
}
```

For clustering with either algorithm, you will need to specify a resolution parameter. This controls how fine or coarse the resulting clusters are; smaller values result in fewer, larger clusters, while higher values result in more smaller clusters. Typically, values between 0.4 and 1.2 will return good results, but it is a good idea to explore a wider range. Seurat handily supports clustering at multiple resolutions and storing the results of each clustering pass as separate metadata columns. In the following code block we set the range of resolution parameter values to between `0.2` and `2.0`, increasing in steps of `0.2` which should cover a large enough range of values for most datasets. Feel free to change this to suit your data.

```{r sct_resolutions}
# Set the range of clustering resolutions to try
min_res <- 0.2
max_res <- 2.0
step_size <- 0.2

res <- seq(min_res, max_res, step_size)
```

Now run the following code block to run both the `SCTransform` normalisation and the clustering of your data.

```{r init_sct_reduce_dims}
options(future.globals.maxSize = 1000*1024^2)

all_init_sct <- lapply(all_filtered_seurat_objects, function(s) {
  # Run SCTransform and find clusters
  s_sct <- SCTransform_reduce_dims(s)
  s_sct <- FindClusters(s_sct, resolution = res, verbose = 0)
  s_sct
})
```

In order to determine the best cluster resolution to use, we can use the `clustree` package to create a clustering tree graph. This summarises how the cells move between clusters when we change the clustering resolution. We are looking for a range of clustering resolutions where the clusters remain relatively stable; in contrast, we want to avoid choosing unstable resolutions that cause large changes in the cluster assignments.

The following block will generate these clustering trees and display them.

```{r init_plot_clustertrees}
all_init_clustrees <- lapply(all_init_sct, function(s) {
  s_name <- as.character(s@meta.data$orig.ident[[1]])
  # Plot cluster trees
  clustree::clustree(s, prefix = "SCT_snn_res.") + ggtitle(s_name)
})

for (p in all_init_clustrees) {
  print(p)
}
```

Running the next block will open up an interactive Shiny app that you can use to inspect each of your clustering resolutions and the effect they have on the data:

```{r init_app_explore_clusters, eval = FALSE}
app_explore_clusters(all_init_sct, all_init_clustrees, res)
```

### Define cluster resolutions and clusters to remove

Based on the cluster tree graphs and the exploratory analysis of the clusters at each resolution, you should now pick a stable cluster resolution for each sample and define any cluster IDs that should be removed. For example, clusters that have high percentage MT counts or consistently low read counts are likely artefacts that you will want to exclude from downstream analyses.

You can specify your cluster resolutions and IDs to remove in the file `inputs/cluster_filter.tsv` that was generated earlier. The file contains three columns: `sample`, `res`, and `clusters_to_remove`, e.g.:

```         
sample  res clusters_to_remove
normal_sample 1.2 15,20
tumour_sample 0.8 16
```

If you have no clusters to remove, simply leave the `clusters_to_remove` column blank.

```{r read_filter_clusters}
cluster_filter_samplesheet <- read_tsv("inputs/cluster_filter.tsv")
cluster_filter_samplesheet
```

The following code block will display the per-cluster count plots for each sample, using the resolutions specified in the above samplesheet.

```{r plot_init_clusters}
for (s in all_init_sct) {
  s_name <- as.character(s@meta.data$orig.ident[[1]])
  s_res <- cluster_filter_samplesheet$res[match(s_name, cluster_filter_samplesheet$sample)]
  s_res_named <- paste0("SCT_snn_res.", s_res)
  p <- s@meta.data %>%
    ggplot(aes(x = nCount_RNA, y = nFeature_RNA, col = percent.mt)) +
      geom_point(size = 0.3) +
      facet_wrap(s_res_named) +
      scale_x_log10() +
      scale_y_log10() +
      theme_light() +
      viridis::scale_color_viridis() +
      annotation_logticks(side = "lb", colour = "lightgrey") +
      ggtitle(paste0(s_name, ": ", s_res_named))
  print(p)
}
```

Now we will apply the cluster filtering to our original Seurat objects.

```{r remove_bad_clusters}
cells_to_keep <- lapply(all_init_sct, function(s) {
  s_name <- as.character(s@meta.data$orig.ident[[1]])
  cluster_res <- as.numeric(cluster_filter_samplesheet$res[match(s_name, cluster_filter_samplesheet$sample)])
  cluster_res <- paste0("SCT_snn_res.", cluster_res)
  stopifnot(cluster_res %in% names(s@meta.data))
  clusters_to_remove <- as.character(cluster_filter_samplesheet$clusters_to_remove[match(s_name, cluster_filter_samplesheet$sample)])
  if (clusters_to_remove == "") {
    return(NULL)
  }
  clusters_to_remove <- as.integer(strsplit(clusters_to_remove, ",")[[1]])
  Idents(s) <- cluster_res
  
  rm_cells <- s@meta.data[[cluster_res]] %in% clusters_to_remove
  keep_cells <- !rm_cells
  keep_cells <- rownames(s@meta.data[keep_cells,])
  return(keep_cells)
})

all_cluster_filtered_seurat_objects <- lapply(all_filtered_seurat_objects, function(s) {
  s_name <- as.character(s@meta.data$orig.ident[[1]])
  
  keep_cells <- cells_to_keep[[s_name]]
  
  if (is.null(keep_cells)) {
    return(s)
  }
  
  s %>% subset(cells = keep_cells)
})

n_cells_post_cluster_filtered <- sapply(all_cluster_filtered_seurat_objects, function(s) { nrow(s@meta.data) })
n_cells_post_cluster_filtered <- tibble(sample = names(n_cells_post_cluster_filtered), n_cells_post_cluster_filtered = n_cells_post_cluster_filtered)

filter_summary <- n_cells_pre_filtered %>% left_join(n_cells_post_filtered) %>% left_join(n_cells_post_cluster_filtered)
filter_summary
```

## Repeat SCTransform and perform final clustering

Now that we have filtered our data, we want to re-run `SCTransform` and clustering, as the filtering process will invalidate the previous round of normalisation and clustering. We will use the same range of clustering resolutions that we defined above in the `res` variable.

```{r sct_reduce_dims}
options(future.globals.maxSize = 1000*1024^2)

all_sct <- lapply(all_cluster_filtered_seurat_objects, function(s) {
  # Run SCTransform and find clusters
  s_sct <- SCTransform_reduce_dims(s)
  s_sct <- FindClusters(s_sct, resolution = res, verbose = 0)
  s_sct
})
```

Again, we generate clustering trees to help us decide on a final resolution to use:

```{r plot_clustertrees}
all_clustrees <- lapply(all_sct, function(s) {
  s_name <- as.character(s@meta.data$orig.ident[[1]])
  # Plot cluster trees
  clustree::clustree(s, prefix = "SCT_snn_res.") + ggtitle(s_name)
})

for (p in all_init_clustrees) {
  print(p)
}
```

We can re-run the interactive Shiny app to help us inspect the data for each sample at each clustering resolution:

```{r app_explore_clusters, eval = FALSE}
app_explore_clusters(all_sct, all_clustrees, res)
```

### Define final cluster resolutions

Once again, based on the cluster tree graphs and the exploratory analysis of the clusters at each resolution, you should now pick a final stable cluster resolution for each sample.

You can specify your cluster resolutions using the TSV file `inputs/cluster_resolutions.tsv` generated earlier. It has two columns: `sample` and `res`, e.g.:

```         
sample  res
normal_sample 1.2
tumour_sample 0.8
```

Note that the `res` column values will likely be different to the values used in `inputs/cluster_filter.tsv`.

```{r filter_clusters}
cluster_res_samplesheet <- read_tsv("inputs/cluster_resolutions.tsv")
cluster_res_samplesheet
```

Plot the per-cluster count plots for each sample at the chosen clustering resolutions:

```{r plot_clusters}
for (s in all_sct) {
  s_name <- as.character(s@meta.data$orig.ident[[1]])
  s_res <- cluster_res_samplesheet$res[match(s_name, cluster_res_samplesheet$sample)]
  s_res_named <- paste0("SCT_snn_res.", s_res)
  p <- s@meta.data %>%
    ggplot(aes(x = nCount_RNA, y = nFeature_RNA, col = percent.mt)) +
      geom_point(size = 0.3) +
      facet_wrap(s_res_named) +
      scale_x_log10() +
      scale_y_log10() +
      theme_light() +
      viridis::scale_color_viridis() +
      annotation_logticks(side = "lb", colour = "lightgrey") +
      ggtitle(paste0(s_name, ": ", s_res_named))
  print(p)
}
```

Finally, apply the chosen clustering resolution by setting each Seurat object's Identity to that resolution.

```{r apply_cluster_resolutions}
all_sct <- lapply(all_sct, function(s) {
  s_name <- as.character(s@meta.data$orig.ident[[1]])
  cluster_res <- as.numeric(cluster_res_samplesheet$res[match(s_name, cluster_res_samplesheet$sample)])
  cluster_res <- paste0("SCT_snn_res.", cluster_res)
  Idents(s) <- cluster_res
  s
})
```

Now that we have a filtered dataset, we can save the data for each sample to a separate `.Rds` file:

```{r save_data, eval = FALSE}
dir.create("outputs")
for (s in all_sct) {
  sample_name <- as.character(s@meta.data$orig.ident[[1]])
  SaveSeuratRds(s, paste0("outputs/", sample_name, ".filtered_clustered.Rds"))
}
```
